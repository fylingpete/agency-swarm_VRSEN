{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fylingpete/agency-swarm_VRSEN/blob/main/Kopie_von_Agency_Swarm_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSR2RQa0Z3V3",
        "outputId": "f8fc5892-7a80-4d7e-b03a-d6278c08ad0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/VRSEN/agency-swarm.git\n",
            "  Cloning https://github.com/VRSEN/agency-swarm.git to /tmp/pip-req-build-bsi8_g6q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/VRSEN/agency-swarm.git /tmp/pip-req-build-bsi8_g6q\n",
            "  Resolved https://github.com/VRSEN/agency-swarm.git to commit 75ff8a6685fb9fc25c27c17aa21ddc85f6815eb5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-3.9.9-py3-none-any.whl (26 kB)\n",
            "Collecting openai==1.3.0 (from agency-swarm==0.1.0)\n",
            "  Downloading openai-1.3.0-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting instructor==0.3.4 (from agency-swarm==0.1.0)\n",
            "  Downloading instructor-0.3.4-py3-none-any.whl (22 kB)\n",
            "Collecting deepdiff==6.7.1 (from agency-swarm==0.1.0)\n",
            "  Downloading deepdiff-6.7.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor==2.3.0 in /usr/local/lib/python3.10/dist-packages (from agency-swarm==0.1.0) (2.3.0)\n",
            "Collecting python-dotenv==1.0.0 (from agency-swarm==0.1.0)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff==6.7.1->agency-swarm==0.1.0)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from instructor==0.3.4->agency-swarm==0.1.0)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting pydantic<3.0.0,>=2.0.2 (from instructor==0.3.4->agency-swarm==0.1.0)\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor==0.3.4->agency-swarm==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.0->agency-swarm==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.0->agency-swarm==0.1.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.0->agency-swarm==0.1.0) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.0->agency-swarm==0.1.0) (4.5.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.0 (from gradio)\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Requirement already satisfied: lxml>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (4.9.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting socksio==1.* (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting h2<5,>=3 (from httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.0.2->instructor==0.3.4->agency-swarm==0.1.0)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic<3.0.0,>=2.0.2->instructor==0.3.4->agency-swarm==0.1.0)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<5,>=4.5 (from openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer<0.10.0,>=0.9.0->instructor==0.3.4->agency-swarm==0.1.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer<0.10.0,>=0.9.0->instructor==0.3.4->agency-swarm==0.1.0)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.9.0->instructor==0.3.4->agency-swarm==0.1.0) (13.7.0)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai==1.3.0->agency-swarm==0.1.0) (1.1.3)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx<1,>=0.23.0->openai==1.3.0->agency-swarm==0.1.0)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer<0.10.0,>=0.9.0->instructor==0.3.4->agency-swarm==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer<0.10.0,>=0.9.0->instructor==0.3.4->agency-swarm==0.1.0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer<0.10.0,>=0.9.0->instructor==0.3.4->agency-swarm==0.1.0) (0.1.2)\n",
            "Building wheels for collected packages: agency-swarm, ffmpy\n",
            "  Building wheel for agency-swarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for agency-swarm: filename=agency_swarm-0.1.0-py3-none-any.whl size=19691 sha256=75b6d5fe91e8891f510721b39922e45a590a991b6ee51eac5184d485231e644e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1d3ieptm/wheels/6a/20/22/d35901ecfae45e915b75f632c889b269b6e9df4eea2770f4b0\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=7d0650b4889e4ce712ff9ad30a921e2e89974c74302213ae555c823bf52804db\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built agency-swarm ffmpy\n",
            "Installing collected packages: pydub, ffmpy, brotli, websockets, typing-extensions, tomlkit, socksio, shellingham, semantic-version, python-multipart, python-dotenv, orjson, ordered-set, hyperframe, hpack, h11, docstring-parser, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, h2, deepdiff, pydantic, httpx, openai, gradio-client, fastapi, instructor, gradio, duckduckgo-search, agency-swarm\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed agency-swarm-0.1.0 aiofiles-23.2.1 annotated-types-0.6.0 brotli-1.1.0 colorama-0.4.6 deepdiff-6.7.1 docstring-parser-0.15 duckduckgo-search-3.9.9 fastapi-0.104.1 ffmpy-0.3.1 gradio-4.7.1 gradio-client-0.7.0 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.2 httpx-0.25.2 hyperframe-6.0.1 instructor-0.3.4 openai-1.3.0 ordered-set-4.1.0 orjson-3.9.10 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-dotenv-1.0.0 python-multipart-0.0.6 semantic-version-2.10.0 shellingham-1.5.4 socksio-1.0.0 starlette-0.27.0 tomlkit-0.12.0 typing-extensions-4.8.0 uvicorn-0.24.0.post1 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/VRSEN/agency-swarm.git gradio duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agency_swarm import set_openai_key\n",
        "from getpass import getpass\n",
        "set_openai_key(getpass(\"Please enter your openai key: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHqVhxU-x1rL",
        "outputId": "0fc7318c-d378-4cbb-c3f3-7900acf43730"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your openai key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CEO Agent\n"
      ],
      "metadata": {
        "id": "1h43Eh0PSDHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ceo_instructions = \"\"\"# Instructions for CEO Agent\n",
        "\n",
        "- Ensure that proposal is send to the user before proceeding with task execution.\n",
        "- Delegate tasks to appropriate agents, ensuring they align with their expertise and capabilities.\n",
        "- Clearly define the objectives and expected outcomes for each task.\n",
        "- Provide necessary context and background information for successful task completion.\n",
        "- Maintain ongoing communication with agents until complete task execution.\n",
        "- Review completed tasks to ensure they meet the set objectives.\n",
        "- Report the results back to the user.\"\"\""
      ],
      "metadata": {
        "id": "w7xwtx3-LWna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agency_swarm import Agent\n",
        "\n",
        "ceo = Agent(name=\"CEO\",\n",
        "            description=\"Responsible for client communication, task planning and management.\",\n",
        "            instructions=ceo_instructions, # can be a file like ./instructions.md\n",
        "            files_folder=None,\n",
        "            tools=[])"
      ],
      "metadata": {
        "id": "NKFbvF54SGT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Virtual Assistant"
      ],
      "metadata": {
        "id": "y_9zs8XLrkal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing tools from langchain example.  \n",
        "You can skip these and remove them from va agent below."
      ],
      "metadata": {
        "id": "pSDsePTyLzkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.318 &> /dev/null"
      ],
      "metadata": {
        "id": "sY8FY_UwLKy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities.zapier import ZapierNLAWrapper\n",
        "from langchain.agents.agent_toolkits import ZapierToolkit\n",
        "import os\n",
        "from langchain.tools import format_tool_to_openai_function\n",
        "from langchain.tools.zapier.tool import ZapierNLARunAction\n",
        "\n",
        "# https://nla.zapier.com/docs/authentication/\n",
        "os.environ[\"ZAPIER_NLA_API_KEY\"] = getpass(\"Your Zapier NLA Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE7zTpfILy5i",
        "outputId": "d3d6b41e-5547-4853-abe6-0bbacd6dc43c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Zapier NLA Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agency_swarm import BaseTool\n",
        "from pydantic import Field\n",
        "\n",
        "actions = ZapierNLAWrapper().list()\n",
        "\n",
        "class FindEmail(BaseTool):\n",
        "    \"\"\"Use this tool to find an email in user's mailbox\"\"\"\n",
        "    instructions: str = Field(..., description=\"The search phrase you want to use to find a relevant email.\")\n",
        "\n",
        "    def run(self):\n",
        "      action = next(\n",
        "          (a for a in actions if a[\"description\"].startswith(\"Gmail: Find Email\")), None\n",
        "      )\n",
        "      return str(ZapierNLARunAction(\n",
        "              action_id=action[\"id\"],\n",
        "              zapier_description=action[\"description\"],\n",
        "              params_schema=action[\"params\"],\n",
        "          ).run(self.search_string))\n",
        "\n",
        "class DraftEmail(BaseTool):\n",
        "    \"\"\"Use this tool to draft a response email\"\"\"\n",
        "    instructions: str = Field(..., description=\"Clearly outline in natural language the content of the email you need to draft and the address of the recepient.\")\n",
        "\n",
        "    def run(self):\n",
        "        action = next(\n",
        "            (a for a in actions if a[\"description\"].startswith(\"Gmail: Create Draft Reply\")), None\n",
        "        )\n",
        "        return str(ZapierNLARunAction(\n",
        "                action_id=action[\"id\"],\n",
        "                zapier_description=action[\"description\"],\n",
        "                params_schema=action[\"params\"],\n",
        "            ).run(self.instructions))"
      ],
      "metadata": {
        "id": "w2OO-KxEuh8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom tools"
      ],
      "metadata": {
        "id": "B5m14T_KhwFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "from agency_swarm.util.oai import get_openai_client\n",
        "\n",
        "client = get_openai_client()\n",
        "\n",
        "\n",
        "class SearchWeb(BaseTool):\n",
        "    \"\"\"Search the web with a search phrase and return the results.\"\"\"\n",
        "\n",
        "    phrase: str = Field(..., description=\"The search phrase you want to use. Optimize the search phrase for an internet search engine.\")\n",
        "\n",
        "    # This code will be executed if the agent calls this tool\n",
        "    def run(self):\n",
        "      with DDGS() as ddgs:\n",
        "        return str([r for r in ddgs.text(self.phrase, max_results=3)])\n",
        "\n",
        "class GenerateProposal(BaseTool):\n",
        "    \"\"\"Generate a proposal for a project based on a project brief. Remember that user does not have access to the output of this function. You must send it back to him after execution.\"\"\"\n",
        "    project_brief: str = Field(..., description=\"The project breif to generate a proposal for.\")\n",
        "\n",
        "    def run(self):\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "              {\"role\": \"system\", \"content\": \"You are a professional proposal drafting assistant. Do not include any actual technologies or technical details into proposal until specified in the project brief. Be short.\"},\n",
        "              {\"role\": \"user\", \"content\": \"Please draft a proposal for the ollowing project brief: \" + self.project_brief}\n",
        "            ]\n",
        "          )\n",
        "\n",
        "        return str(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "MxkO2GHnNekC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "va_instructions = \"\"\"### Instructions for Virtual Assistant\n",
        "\n",
        "Your role is to assist users in executing tasks like below. If the task is outside of your capabilities, please report back to the user.\n",
        "\n",
        "#### 1. Drafting Emails\n",
        "   - **Understand Context and Tone**: Familiarize yourself with the context of each email. Maintain a professional and courteous tone.\n",
        "   - **Accuracy and Clarity**: Ensure that the information is accurate and presented clearly. Avoid jargon unless it's appropriate for the recipient.\n",
        "\n",
        "#### 2. Generating Proposals\n",
        "   - **Gather Requirements**: Collect all necessary information about the project, including client needs, objectives, and any specific requests.\n",
        "\n",
        "#### 3. Conducting Research\n",
        "   - **Understand the Objective**: Clarify the purpose and objectives of the research to focus on relevant information.\n",
        "   - **Summarize Findings**: Provide clear, concise summaries of the research findings, highlighting key points and how they relate to the project or inquiry.\n",
        "   - **Cite Sources**: Properly cite all sources to maintain integrity and avoid plagiarism.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8u4x6gdeSARg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "va = Agent(name=\"Virtual Assistant\",\n",
        "            description=\"Responsible for drafting emails, doing research and writing proposals.\",\n",
        "            instructions=va_instructions,\n",
        "            files_folder=None,\n",
        "            tools=[SearchWeb, FindEmail, DraftEmail, GenerateProposal])"
      ],
      "metadata": {
        "id": "6e8aWSRRBlBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developer Agent"
      ],
      "metadata": {
        "id": "x0zLIv1i75gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agency_swarm.tools import BaseTool\n",
        "from pydantic import Field, BaseModel\n",
        "import subprocess\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class ExecuteCommand(BaseTool):\n",
        "    \"\"\"Run any command from the terminal. If there are too many logs, the outputs might be truncated.\"\"\"\n",
        "    command: str = Field(\n",
        "        ..., description=\"The command to be executed.\"\n",
        "    )\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Executes the given command and captures its output and errors.\"\"\"\n",
        "        try:\n",
        "            # Splitting the command into a list of arguments\n",
        "            command_args = self.command.split()\n",
        "\n",
        "            # Executing the command\n",
        "            result = subprocess.run(\n",
        "                command_args,\n",
        "                text=True,\n",
        "                capture_output=True,\n",
        "                check=True\n",
        "            )\n",
        "            return result.stdout\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            return f\"An error occurred: {e.stderr}\"\n",
        "\n",
        "class File(BaseTool):\n",
        "    \"\"\"\n",
        "    File to be written to the disk with an appropriate name and file path, containing code that can be saved and executed locally at a later time.\n",
        "    \"\"\"\n",
        "    file_name: str = Field(\n",
        "        ..., description=\"The name of the file including the extension and the file path from your current directory if needed.\"\n",
        "    )\n",
        "    body: str = Field(..., description=\"Correct contents of a file\")\n",
        "\n",
        "    def run(self):\n",
        "        # Extract the directory path from the file name\n",
        "        directory = os.path.dirname(self.file_name)\n",
        "\n",
        "        # If the directory is not empty, check if it exists and create it if not\n",
        "        if directory and not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # Write the file\n",
        "        with open(self.file_name, \"w\") as f:\n",
        "            f.write(self.body)\n",
        "\n",
        "        return \"File written to \" + self.file_name\n",
        "\n",
        "class Program(BaseTool):\n",
        "    \"\"\"\n",
        "    Set of files that represent a complete and correct program. This environment has access to all standard Python packages and the internet.\n",
        "    \"\"\"\n",
        "    chain_of_thought: str = Field(...,\n",
        "        description=\"Think step by step to determine the correct actions that are needed to implement the program.\")\n",
        "    files: List[File] = Field(..., description=\"List of files\")\n",
        "\n",
        "    def run(self):\n",
        "      outputs = []\n",
        "      for file in self.files:\n",
        "        outputs.append(file.run())\n",
        "\n",
        "      return str(outputs)"
      ],
      "metadata": {
        "id": "aIhhoDWx8BZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_instructions = \"\"\"# Instructions for AI Developer Agent\n",
        "\n",
        "- Write clean and efficient Python code.\n",
        "- Structure your code logically, with `main.py` as the entry point.\n",
        "- Ensure correct imports according to program structure.\n",
        "- Execute your code to test for functionality and errors, before reporting back to the user.\n",
        "- Anticipate and handle potential runtime errors.\n",
        "- Provide clear error messages for easier troubleshooting.\n",
        "- Debug any issues before reporting the results back to the user.\"\"\""
      ],
      "metadata": {
        "id": "lStK381cVF7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agency_swarm.tools import Retrieval, CodeInterpreter\n",
        "\n",
        "dev = Agent(name=\"Developer\",\n",
        "            description=\"Responsible for running and executing Python Programs.\",\n",
        "            instructions=dev_instructions,\n",
        "            files_folder=None,\n",
        "            tools=[ExecuteCommand, Program])\n"
      ],
      "metadata": {
        "id": "7nAlYUHd74pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Agency"
      ],
      "metadata": {
        "id": "8kXMaOqSSb_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agency_manifesto = \"\"\"# \"VRSEN AI\" Agency Manifesto\n",
        "\n",
        "You are a part of a virtual AI development agency called \"VRSEN AI\"\n",
        "\n",
        "Your mission is to empower businesses to navigate the AI revolution successfully.\"\"\""
      ],
      "metadata": {
        "id": "7rer151XX8Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agency_swarm import Agency\n",
        "\n",
        "agency = Agency([\n",
        "    ceo,\n",
        "    [ceo, dev],\n",
        "    [ceo, va],\n",
        "    [dev, va]\n",
        "], shared_instructions=agency_manifesto)"
      ],
      "metadata": {
        "id": "mr2apzHySegB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo with Gradio"
      ],
      "metadata": {
        "id": "I2CHn1B7ShEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agency.demo_gradio(height=900)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "vpGRiEeulQZv",
        "outputId": "bafba759-5ab3-4486-ded7-3a2c5db6051e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fea0b63839a2d22870.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fea0b63839a2d22870.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}